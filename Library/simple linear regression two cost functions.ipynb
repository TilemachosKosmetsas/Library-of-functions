{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "381dc313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9cdd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1.0, 2.0,3.0]) \n",
    "y_train = np.array([4.5, 6.0,6.5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2afc3dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b):\n",
    "   \n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1 / (2 * m) * cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0e3dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost2(x, y, w, b):\n",
    "   \n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        cost = cost + abs( (-w*x[i]+y[i]-b))/(math.sqrt(w**2+1))  \n",
    "    total_cost = (1 / 2*m) * cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e8b6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_cost(x_train, y_train, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c5e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_cost2(x_train, y_train, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "503cc4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b \n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f136b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient2(x, y, w, b): \n",
    "\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        dj_dw_i= ((w**2+1)*x[i]*(b+w*x[i]-y[i])-w*(b+w*x[i]-y[i])**2)/(((w**2+1)**(3/2))*abs(b+w*x[i]-y[i]))\n",
    "        dj_db_i=(b+w*x[i]-y[i])/((math.sqrt(w**2+1))*(abs(b+w*x[i]-y[i])))\n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / 2*m \n",
    "    dj_db = dj_db / 2*m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04674454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.333333333333333, -2.6666666666666665)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient(x_train, y_train, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be750277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10.60660171779821, -3.181980515339464)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient2(x_train, y_train, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5db7e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \n",
    "    \n",
    "    \n",
    "    w = copy.deepcopy(w_in) # avoid modifying global w_in\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Update Parameters using equation (3) above\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daf0d38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 3.24e+00  dj_dw: -5.333e+00, dj_db: -2.667e+00   w:  1.053e+00, b: 1.02667e+00\n",
      "Iteration 1000: Cost 6.00e-02  dj_dw:  3.549e-02, dj_db: -8.067e-02   w:  1.295e+00, b: 2.99632e+00\n",
      "Iteration 2000: Cost 3.07e-02  dj_dw:  1.066e-02, dj_db: -2.423e-02   w:  1.089e+00, b: 3.46530e+00\n",
      "Iteration 3000: Cost 2.80e-02  dj_dw:  3.202e-03, dj_db: -7.279e-03   w:  1.027e+00, b: 3.60618e+00\n",
      "Iteration 4000: Cost 2.78e-02  dj_dw:  9.618e-04, dj_db: -2.186e-03   w:  1.008e+00, b: 3.64850e+00\n",
      "Iteration 5000: Cost 2.78e-02  dj_dw:  2.889e-04, dj_db: -6.568e-04   w:  1.002e+00, b: 3.66121e+00\n",
      "Iteration 6000: Cost 2.78e-02  dj_dw:  8.679e-05, dj_db: -1.973e-04   w:  1.001e+00, b: 3.66503e+00\n",
      "Iteration 7000: Cost 2.78e-02  dj_dw:  2.607e-05, dj_db: -5.926e-05   w:  1.000e+00, b: 3.66617e+00\n",
      "Iteration 8000: Cost 2.78e-02  dj_dw:  7.831e-06, dj_db: -1.780e-05   w:  1.000e+00, b: 3.66652e+00\n",
      "Iteration 9000: Cost 2.78e-02  dj_dw:  2.352e-06, dj_db: -5.347e-06   w:  1.000e+00, b: 3.66662e+00\n",
      "(w,b) found by gradient descent: (  1.0000,  3.6667)\n"
     ]
    }
   ],
   "source": [
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, 1, 1, 0.01, \n",
    "                                                    10000, compute_cost, compute_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc1aaeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 7.31e+00  dj_dw: -1.061e+01, dj_db: -3.182e+00   w:  1.106e+00, b: 1.03182e+00\n",
      "Iteration 1000: Cost 5.46e-01  dj_dw: -2.711e-01, dj_db: -1.057e+00   w:  1.010e+00, b: 3.49745e+00\n",
      "Iteration 2000: Cost 6.24e-01  dj_dw:  1.838e+00, dj_db:  1.056e+00   w:  9.911e-01, b: 3.48918e+00\n",
      "Iteration 3000: Cost 6.58e-01  dj_dw: -6.704e+00, dj_db: -3.197e+00   w:  1.058e+00, b: 3.52342e+00\n",
      "Iteration 4000: Cost 6.21e-01  dj_dw:  1.830e+00, dj_db:  1.051e+00   w:  9.991e-01, b: 3.47331e+00\n",
      "Iteration 5000: Cost 6.55e-01  dj_dw: -6.679e+00, dj_db: -3.185e+00   w:  1.065e+00, b: 3.50749e+00\n",
      "Iteration 6000: Cost 6.17e-01  dj_dw:  1.725e+00, dj_db:  1.027e+00   w:  1.048e+00, b: 3.49937e+00\n",
      "Iteration 7000: Cost 5.76e-01  dj_dw:  1.763e+00, dj_db:  1.036e+00   w:  1.030e+00, b: 3.49137e+00\n",
      "Iteration 8000: Cost 5.45e-01  dj_dw:  1.802e+00, dj_db:  1.045e+00   w:  1.011e+00, b: 3.48326e+00\n",
      "Iteration 9000: Cost 5.51e-01  dj_dw: -2.714e-01, dj_db: -1.055e+00   w:  1.014e+00, b: 3.49608e+00\n",
      "(w,b) found by gradient descent: (  1.0133,  3.4983)\n"
     ]
    }
   ],
   "source": [
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, 1, 1, 0.01, \n",
    "                                                    10000, compute_cost2, compute_gradient2)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d513946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
